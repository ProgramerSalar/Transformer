{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset                    # we will use the dataset \n",
    "from tokenizers import Tokenizer                      # tokenizer library in huggingface\n",
    "from tokenizers.models import WordLevel                # we will use the word lavel tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer        # that class will train the tokenizer \n",
    "from tokenizers.pre_tokenizers import Whitespace         # we will split the word according to the white space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tokenizers\n",
    "- Most of the code taken from https://huggingface.co/docs/tokenizers/quicktour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get all senteces so that we can iterate through the dataset to get all the sentences corresponding to the part the particular language \n",
    "# for which we are creating the tokenizer \n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:    # each item of dataset is pair of sentences extract the one particular language \n",
    "        yield item['translation'][lang]   # this is the item representing the pair and from this pair we extract only the one language thats we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config,     # configuration of our model \n",
    "                           ds,          # dataset \n",
    "                           lang):        # language which language convert the token\n",
    "    \n",
    "    # that file where we will save this tokenizer \n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))   # path of configuration file name is tokenizer_file and which format of language\n",
    "    if not Path.exists(tokenizer_path):             # if path is exists \n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))    # unk means unknown, if our tokenizer sees a word that it does not recogized in its vocabulary it will replace it with this word unknown it will map it to the number corresponding to this word unknown \n",
    "        tokenizer.pre_tokenizer = Whitespace           # that we wplit by white space \n",
    "        \n",
    "        # then we train we build the trainer to train our tokenizer \n",
    "        trainer = WordLevelTrainer(   # this is word lavel trainer it will split the word using the white space and also single word\n",
    "            special_tokens = [\"[UNK]\",     # unknown word \n",
    "                              \"[PAD]\",     # padding of word \n",
    "                              \"[SOS]\",     # start of the sentence \n",
    "                              \"[EOS]\"],    # end of the sentence it has to have a frequency of at least two \n",
    "            min_frequency=2)   # that word for a word to appear in our vocabulary \n",
    "        \n",
    "        tokenizer.train_from_iterator(get_all_sentences(  #  give all the sentences from dataset \n",
    "            ds,         # dataset \n",
    "            lang),      # language \n",
    "            trainer=trainer)   # word lavel trainer \n",
    "        tokenizer.save(str(tokenizer_path))    # save the tokenizer_path\n",
    "        \n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from dataset import BilingualDataset, causal_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note- in this line \n",
    "    ```train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])```\n",
    "\n",
    "    we will create the dataset that is BilingualDataset dataset, so you go to dataset.py file see BilingualDataset dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(config):  # get dataset which also data takes the configuation of the model\n",
    "    \n",
    "    # it only has the train split, so we divide it overselves \n",
    "    # huggingface allow us to download its very easily we just need to tell him what is the name of the dataset and tell him what is the subset we want \n",
    "    # the subset that is English to italian but we want to also make it configurable for you guys to change the language very fast so \n",
    "    ds_raw = load_dataset(  \n",
    "        'opus_books',   # what is the name of the dataset \n",
    "        f\"{config['lang_src']}-{config['lang_tgt']}\",      # subset of the dataset, we will have two parameters in the configuration one is called language source and one is called language target\n",
    "        split='train')   # leter defined what split we want in dataset in our case only training split we want \n",
    "    \n",
    "    \n",
    "    # Build tokenizers \n",
    "    tokenizer_src = get_or_build_tokenizer(config,    # configuration \n",
    "                                           ds_raw,    # Raw dataset\n",
    "                                           config['lang_src'])  # source language of tokenizer \n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "    \n",
    "    \n",
    "    # keep 90% for training 10% for validation \n",
    "    train_ds_size = int(0.9 * len(ds_raw))          # this is train data \n",
    "    val_ds_size = len(ds_raw) - train_ds_size       # this is validatio data \n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_raw, val_ds_raw])   # the method random split allow it's a method from pytorch that allow to split a dataset using the size that we give as input\n",
    "    \n",
    "    \n",
    "    # we need to create the dataset the dataset theat our model will use to access the tensor directory because before we just created the tokenizer and we just loaded the data but \n",
    "    # we need to create the tensors that our model will use \n",
    "    # create the BilingualDataset dataset \n",
    "    \n",
    "    \n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    \n",
    "    \n",
    "    # find the maximum length of each sentence in the source and target sentence \n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    \n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids \n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids \n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "        \n",
    "        \n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "    \n",
    "    \n",
    "    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
