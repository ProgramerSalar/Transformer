{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset                    # we will use the dataset \n",
    "from tokenizers import Tokenizer                      # tokenizer library in huggingface\n",
    "from tokenizers.models import WordLevel                # we will use the word lavel tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer        # that class will train the tokenizer \n",
    "from tokenizers.pre_tokenizers import Whitespace         # we will split the word according to the white space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tokenizers\n",
    "- Most of the code taken from https://huggingface.co/docs/tokenizers/quicktour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get all senteces so that we can iterate through the dataset to get all the sentences corresponding to the part the particular language \n",
    "# for which we are creating the tokenizer \n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:    # each item of dataset is pair of sentences extract the one particular language \n",
    "        yield item['translation'][lang]   # this is the item representing the pair and from this pair we extract only the one language thats we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config,     # configuration of our model \n",
    "                           ds,          # dataset \n",
    "                           lang):        # language which language convert the token\n",
    "    \n",
    "    # that file where we will save this tokenizer \n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))   # path of configuration file name is tokenizer_file and which format of language\n",
    "    if not Path.exists(tokenizer_path):             # if path is exists \n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))    # unk means unknown, if our tokenizer sees a word that it does not recogized in its vocabulary it will replace it with this word unknown it will map it to the number corresponding to this word unknown \n",
    "        tokenizer.pre_tokenizer = Whitespace           # that we wplit by white space \n",
    "        \n",
    "        # then we train we build the trainer to train our tokenizer \n",
    "        trainer = WordLevelTrainer(   # this is word lavel trainer it will split the word using the white space and also single word\n",
    "            special_tokens = [\"[UNK]\",     # unknown word \n",
    "                              \"[PAD]\",     # padding of word \n",
    "                              \"[SOS]\",     # start of the sentence \n",
    "                              \"[EOS]\"],    # end of the sentence it has to have a frequency of at least two \n",
    "            min_frequency=2)   # that word for a word to appear in our vocabulary \n",
    "        \n",
    "        tokenizer.train_from_iterator(get_all_sentences(  #  give all the sentences from dataset \n",
    "            ds,         # dataset \n",
    "            lang),      # language \n",
    "            trainer=trainer)   # word lavel trainer \n",
    "        tokenizer.save(str(tokenizer_path))    # save the tokenizer_path\n",
    "        \n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
