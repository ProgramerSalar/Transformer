{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Input Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnputEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model:int,\n",
    "                 vocab_size:int\n",
    "                 )-> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "\n",
    "        # calculate the embeddings \n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.embeddings(x)  * math.sqrt(self.d_model)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnputEmbedding(\n",
       "  (embeddings): Embedding(20000, 4)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 4\n",
    "vocab_size = 20000\n",
    "\n",
    "embedding_layer = EnputEmbedding(d_model, \n",
    "                                 vocab_size\n",
    "                                 )\n",
    "embedding_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<style>\n",
    "    .blue {\n",
    "        background-color: #0074D9;\n",
    "        border-radius:2px\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"blue\">\n",
    "    Question: what is the difference  between vocab size and sequence length in transformer, let's give the example ?\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sure, let's consider the sentence \"Ram eats mango.\"\n",
    "\n",
    "**Vocabulary Size**: If we tokenize this sentence into words, our vocabulary (the set of unique tokens) would be [\"Ram\", \"eats\", \"mango\"]. The vocabulary size is 3 because there are three unique tokens.\n",
    "\n",
    "**Sequence Length**: The sequence length of the sentence \"Ram eats mango\" is 3 because there are three tokens in this sequence when we process it.\n",
    "\n",
    "In a transformer model, the embedding layer would have an embedding for each token in the vocabulary, and the model would process sequences of tokens up to a certain sequence length. For instance, if the model's maximum sequence length is 5, it can process sequences like \"Ram eats mango\" (length 3) without any issue, but for longer sequences like \"Ram eats a ripe mango under the tree\" (length 7), it would need to truncate or otherwise handle the extra tokens since the sequence length exceeds the model's maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .blue {\n",
    "        background-color: #0074D9;\n",
    "        border-radius:2px;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"blue\">\n",
    "    Question: suppose that sentence is [Ram eats mango give this mango by ram] then what is vocabulary size and sequence length of this sentence ?\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For the sentence \"Ram eats mango give this mango by ram\":\n",
    "\n",
    "**Vocabulary Size**: The unique words are [\"Ram\", \"eats\", \"mango\", \"give\", \"this\", \"by\"], noting that \"Ram\" and \"ram\" are considered the same if we ignore case sensitivity. So, the vocabulary size is 6.\n",
    "\n",
    "**Sequence Length**: The total number of words in the sentence is 8, so the sequence length is 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 d_model:int,\n",
    "                 seq_len:int,\n",
    "                 dropout:float\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # calculate the positional Encoding\n",
    "\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "\n",
    "\n",
    "        # calculate the sine indices \n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "\n",
    "        # calculate the cos indices \n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "        # unsqueeze the Encoding \n",
    "        pe.unsqueeze(0)\n",
    "\n",
    "\n",
    "        # register the Encoding using register buffer \n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad(False)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PositionalEncoding(\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(\n",
    "    d_model, \n",
    "    seq_len=20,\n",
    "    dropout=0.2\n",
    ")\n",
    "pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the output `PositionalEncoding((dropout): Dropout(p=0.2, inplace=False))`, the `p` represents the probability of an element to be zeroed, which is the dropout rate. So, `p=0.2` means that there is a 20% chance that any given neuron (or element) will be set to zero during training at each update cycle. This helps prevent overfitting by providing a form of regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.6466e-01, 9.3057e-01, 8.9769e-01, 8.6596e-01, 8.3536e-01,\n",
       "        8.0584e-01, 7.7737e-01, 7.4989e-01, 7.2339e-01, 6.9783e-01, 6.7317e-01,\n",
       "        6.4938e-01, 6.2643e-01, 6.0430e-01, 5.8294e-01, 5.6234e-01, 5.4247e-01,\n",
       "        5.2330e-01, 5.0481e-01, 4.8697e-01, 4.6976e-01, 4.5316e-01, 4.3714e-01,\n",
       "        4.2170e-01, 4.0679e-01, 3.9242e-01, 3.7855e-01, 3.6517e-01, 3.5227e-01,\n",
       "        3.3982e-01, 3.2781e-01, 3.1623e-01, 3.0505e-01, 2.9427e-01, 2.8387e-01,\n",
       "        2.7384e-01, 2.6416e-01, 2.5483e-01, 2.4582e-01, 2.3714e-01, 2.2876e-01,\n",
       "        2.2067e-01, 2.1288e-01, 2.0535e-01, 1.9810e-01, 1.9110e-01, 1.8434e-01,\n",
       "        1.7783e-01, 1.7154e-01, 1.6548e-01, 1.5963e-01, 1.5399e-01, 1.4855e-01,\n",
       "        1.4330e-01, 1.3824e-01, 1.3335e-01, 1.2864e-01, 1.2409e-01, 1.1971e-01,\n",
       "        1.1548e-01, 1.1140e-01, 1.0746e-01, 1.0366e-01, 1.0000e-01, 9.6466e-02,\n",
       "        9.3057e-02, 8.9769e-02, 8.6596e-02, 8.3536e-02, 8.0584e-02, 7.7736e-02,\n",
       "        7.4989e-02, 7.2339e-02, 6.9783e-02, 6.7317e-02, 6.4938e-02, 6.2643e-02,\n",
       "        6.0430e-02, 5.8294e-02, 5.6234e-02, 5.4247e-02, 5.2330e-02, 5.0481e-02,\n",
       "        4.8697e-02, 4.6976e-02, 4.5316e-02, 4.3714e-02, 4.2170e-02, 4.0679e-02,\n",
       "        3.9242e-02, 3.7855e-02, 3.6517e-02, 3.5227e-02, 3.3982e-02, 3.2781e-02,\n",
       "        3.1623e-02, 3.0505e-02, 2.9427e-02, 2.8387e-02, 2.7384e-02, 2.6416e-02,\n",
       "        2.5483e-02, 2.4582e-02, 2.3714e-02, 2.2876e-02, 2.2067e-02, 2.1288e-02,\n",
       "        2.0535e-02, 1.9810e-02, 1.9110e-02, 1.8434e-02, 1.7783e-02, 1.7154e-02,\n",
       "        1.6548e-02, 1.5963e-02, 1.5399e-02, 1.4855e-02, 1.4330e-02, 1.3824e-02,\n",
       "        1.3335e-02, 1.2864e-02, 1.2409e-02, 1.1971e-02, 1.1548e-02, 1.1140e-02,\n",
       "        1.0746e-02, 1.0366e-02, 1.0000e-02, 9.6466e-03, 9.3057e-03, 8.9769e-03,\n",
       "        8.6596e-03, 8.3536e-03, 8.0584e-03, 7.7736e-03, 7.4989e-03, 7.2339e-03,\n",
       "        6.9783e-03, 6.7317e-03, 6.4938e-03, 6.2643e-03, 6.0430e-03, 5.8294e-03,\n",
       "        5.6234e-03, 5.4247e-03, 5.2330e-03, 5.0481e-03, 4.8697e-03, 4.6976e-03,\n",
       "        4.5316e-03, 4.3714e-03, 4.2170e-03, 4.0679e-03, 3.9242e-03, 3.7855e-03,\n",
       "        3.6517e-03, 3.5227e-03, 3.3982e-03, 3.2781e-03, 3.1623e-03, 3.0505e-03,\n",
       "        2.9427e-03, 2.8387e-03, 2.7384e-03, 2.6416e-03, 2.5483e-03, 2.4582e-03,\n",
       "        2.3714e-03, 2.2876e-03, 2.2067e-03, 2.1288e-03, 2.0535e-03, 1.9810e-03,\n",
       "        1.9110e-03, 1.8434e-03, 1.7783e-03, 1.7154e-03, 1.6548e-03, 1.5963e-03,\n",
       "        1.5399e-03, 1.4855e-03, 1.4330e-03, 1.3824e-03, 1.3335e-03, 1.2864e-03,\n",
       "        1.2409e-03, 1.1971e-03, 1.1548e-03, 1.1140e-03, 1.0746e-03, 1.0366e-03,\n",
       "        1.0000e-03, 9.6466e-04, 9.3057e-04, 8.9769e-04, 8.6596e-04, 8.3536e-04,\n",
       "        8.0584e-04, 7.7736e-04, 7.4989e-04, 7.2339e-04, 6.9783e-04, 6.7317e-04,\n",
       "        6.4938e-04, 6.2643e-04, 6.0430e-04, 5.8294e-04, 5.6234e-04, 5.4247e-04,\n",
       "        5.2330e-04, 5.0481e-04, 4.8697e-04, 4.6976e-04, 4.5316e-04, 4.3714e-04,\n",
       "        4.2170e-04, 4.0679e-04, 3.9242e-04, 3.7855e-04, 3.6517e-04, 3.5227e-04,\n",
       "        3.3982e-04, 3.2781e-04, 3.1623e-04, 3.0505e-04, 2.9427e-04, 2.8387e-04,\n",
       "        2.7384e-04, 2.6416e-04, 2.5483e-04, 2.4582e-04, 2.3714e-04, 2.2876e-04,\n",
       "        2.2067e-04, 2.1288e-04, 2.0535e-04, 1.9810e-04, 1.9110e-04, 1.8434e-04,\n",
       "        1.7783e-04, 1.7154e-04, 1.6548e-04, 1.5963e-04, 1.5399e-04, 1.4855e-04,\n",
       "        1.4330e-04, 1.3824e-04, 1.3335e-04, 1.2864e-04, 1.2409e-04, 1.1971e-04,\n",
       "        1.1548e-04, 1.1140e-04, 1.0746e-04, 1.0366e-04])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512\n",
    "\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000) / d_model))\n",
    "div_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 9.6466e-01, 9.3057e-01, 8.9769e-01, 8.6596e-01, 8.3536e-01,\n",
       "        8.0584e-01, 7.7737e-01, 7.4989e-01, 7.2339e-01, 6.9783e-01, 6.7317e-01,\n",
       "        6.4938e-01, 6.2643e-01, 6.0430e-01, 5.8294e-01, 5.6234e-01, 5.4247e-01,\n",
       "        5.2330e-01, 5.0481e-01, 4.8697e-01, 4.6976e-01, 4.5316e-01, 4.3714e-01,\n",
       "        4.2170e-01, 4.0679e-01, 3.9242e-01, 3.7855e-01, 3.6517e-01, 3.5227e-01,\n",
       "        3.3982e-01, 3.2781e-01, 3.1623e-01, 3.0505e-01, 2.9427e-01, 2.8387e-01,\n",
       "        2.7384e-01, 2.6416e-01, 2.5483e-01, 2.4582e-01, 2.3714e-01, 2.2876e-01,\n",
       "        2.2067e-01, 2.1288e-01, 2.0535e-01, 1.9810e-01, 1.9110e-01, 1.8434e-01,\n",
       "        1.7783e-01, 1.7154e-01, 1.6548e-01, 1.5963e-01, 1.5399e-01, 1.4855e-01,\n",
       "        1.4330e-01, 1.3824e-01, 1.3335e-01, 1.2864e-01, 1.2409e-01, 1.1971e-01,\n",
       "        1.1548e-01, 1.1140e-01, 1.0746e-01, 1.0366e-01, 1.0000e-01, 9.6466e-02,\n",
       "        9.3057e-02, 8.9769e-02, 8.6596e-02, 8.3536e-02, 8.0584e-02, 7.7736e-02,\n",
       "        7.4989e-02, 7.2339e-02, 6.9783e-02, 6.7317e-02, 6.4938e-02, 6.2643e-02,\n",
       "        6.0430e-02, 5.8294e-02, 5.6234e-02, 5.4247e-02, 5.2330e-02, 5.0481e-02,\n",
       "        4.8697e-02, 4.6976e-02, 4.5316e-02, 4.3714e-02, 4.2170e-02, 4.0679e-02,\n",
       "        3.9242e-02, 3.7855e-02, 3.6517e-02, 3.5227e-02, 3.3982e-02, 3.2781e-02,\n",
       "        3.1623e-02, 3.0505e-02, 2.9427e-02, 2.8387e-02, 2.7384e-02, 2.6416e-02,\n",
       "        2.5483e-02, 2.4582e-02, 2.3714e-02, 2.2876e-02, 2.2067e-02, 2.1288e-02,\n",
       "        2.0535e-02, 1.9810e-02, 1.9110e-02, 1.8434e-02, 1.7783e-02, 1.7154e-02,\n",
       "        1.6548e-02, 1.5963e-02, 1.5399e-02, 1.4855e-02, 1.4330e-02, 1.3824e-02,\n",
       "        1.3335e-02, 1.2864e-02, 1.2409e-02, 1.1971e-02, 1.1548e-02, 1.1140e-02,\n",
       "        1.0746e-02, 1.0366e-02, 1.0000e-02, 9.6466e-03, 9.3057e-03, 8.9769e-03,\n",
       "        8.6596e-03, 8.3536e-03, 8.0584e-03, 7.7736e-03, 7.4989e-03, 7.2339e-03,\n",
       "        6.9783e-03, 6.7317e-03, 6.4938e-03, 6.2643e-03, 6.0430e-03, 5.8294e-03,\n",
       "        5.6234e-03, 5.4247e-03, 5.2330e-03, 5.0481e-03, 4.8697e-03, 4.6976e-03,\n",
       "        4.5316e-03, 4.3714e-03, 4.2170e-03, 4.0679e-03, 3.9242e-03, 3.7855e-03,\n",
       "        3.6517e-03, 3.5227e-03, 3.3982e-03, 3.2781e-03, 3.1623e-03, 3.0505e-03,\n",
       "        2.9427e-03, 2.8387e-03, 2.7384e-03, 2.6416e-03, 2.5483e-03, 2.4582e-03,\n",
       "        2.3714e-03, 2.2876e-03, 2.2067e-03, 2.1288e-03, 2.0535e-03, 1.9810e-03,\n",
       "        1.9110e-03, 1.8434e-03, 1.7783e-03, 1.7154e-03, 1.6548e-03, 1.5963e-03,\n",
       "        1.5399e-03, 1.4855e-03, 1.4330e-03, 1.3824e-03, 1.3335e-03, 1.2864e-03,\n",
       "        1.2409e-03, 1.1971e-03, 1.1548e-03, 1.1140e-03, 1.0746e-03, 1.0366e-03,\n",
       "        1.0000e-03, 9.6466e-04, 9.3057e-04, 8.9769e-04, 8.6596e-04, 8.3536e-04,\n",
       "        8.0584e-04, 7.7736e-04, 7.4989e-04, 7.2339e-04, 6.9783e-04, 6.7317e-04,\n",
       "        6.4938e-04, 6.2643e-04, 6.0430e-04, 5.8294e-04, 5.6234e-04, 5.4247e-04,\n",
       "        5.2330e-04, 5.0481e-04, 4.8697e-04, 4.6976e-04, 4.5316e-04, 4.3714e-04,\n",
       "        4.2170e-04, 4.0679e-04, 3.9242e-04, 3.7855e-04, 3.6517e-04, 3.5227e-04,\n",
       "        3.3982e-04, 3.2781e-04, 3.1623e-04, 3.0505e-04, 2.9427e-04, 2.8387e-04,\n",
       "        2.7384e-04, 2.6416e-04, 2.5483e-04, 2.4582e-04, 2.3714e-04, 2.2876e-04,\n",
       "        2.2067e-04, 2.1288e-04, 2.0535e-04, 1.9810e-04, 1.9110e-04, 1.8434e-04,\n",
       "        1.7783e-04, 1.7154e-04, 1.6548e-04, 1.5963e-04, 1.5399e-04, 1.4855e-04,\n",
       "        1.4330e-04, 1.3824e-04, 1.3335e-04, 1.2864e-04, 1.2409e-04, 1.1971e-04,\n",
       "        1.1548e-04, 1.1140e-04, 1.0746e-04, 1.0366e-04])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512\n",
    "\n",
    "div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/ d_model))\n",
    "div_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .blue {\n",
    "        background-color: #0074D9;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"blue\">\n",
    "   Question: suppose that sentence is [Ram eats mango give this mango by ram] then what is vocabulary size and sequence length of this sentence ?\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "- what is the difference  of output if i use the float in the code i am privide the code above\n",
    "\n",
    "Using `.float()` in the code ensures that the tensor elements are floating-point numbers, which allows for proper floating-point arithmetic during the calculation of `div_term`. If you don't use `.float()`, the tensor elements would be integers, and the operation could result in an error because PyTorch expects tensors to be of the same type when performing element-wise operations. Even if it didn't result in an error, you would get incorrect results due to integer division truncating the decimal part of the results.\n",
    "\n",
    "So, using `.float()` is crucial for obtaining the correct `div_term` values as intended for positional encoding in a transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .blue {\n",
    "        background-color: #0074D9;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"blue\">\n",
    "    Question: why in positional Encoding using the register buffer ?\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In PyTorch, `register_buffer` is used to register a tensor as a buffer within a module. Buffers are tensors that are not treated as model parameters; that is, they do not get updated during training when using optimizers. However, they are part of the model's state and are included when saving and loading the model.\n",
    "\n",
    "Positional encodings in transformers are usually fixed and do not change during training, which makes them perfect candidates for buffers. By registering positional encodings as buffers, you ensure that they're saved and loaded with the model, and they're moved to the correct device along with the rest of the model. This is important for maintaining consistency in the model's behavior, especially when you're deploying it or resuming training from a checkpoint.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<style>\n",
    "    .blue {\n",
    "        background-color: #0074D9;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"blue\">\n",
    "    Question: if i am not used to the register buffer  then what is problem i phase ?\n",
    "</div>\n",
    "\n",
    "\n",
    "If you do not use `register_buffer` for positional encodings in a transformer model, you might face several issues:\n",
    "\n",
    "1. **State Dict**: The positional encodings will not be part of the model's `state_dict`, which means they won't be saved or loaded correctly with the model's parameters. This can lead to inconsistencies when saving and loading models.\n",
    "\n",
    "2. **Device Management**: When you move your model to a different device (e.g., from CPU to GPU), the positional encodings will not automatically move with it. This can cause device mismatches and result in runtime errors.\n",
    "\n",
    "3. **Model Updates**: If the positional encodings are mistakenly treated as parameters rather than buffers, they might get updated during training, which is not desired since positional encodings should remain constant.\n",
    "\n",
    "Using `register_buffer` ensures that the positional encodings are properly managed throughout the lifecycle of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .blue {\n",
    "        background-color: #0074D9;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"blue\">\n",
    "    Question: why i used the requuires_grad if i am not use then what happend ?\n",
    "</div>\n",
    "\n",
    "\n",
    "In PyTorch, `.requires_grad(False)` is used to tell the framework that gradients should not be calculated for this tensor during backpropagation. Positional encodings are constant values that do not need to be updated during training, so setting `.requires_grad` to `False` prevents them from being part of the gradient computation.\n",
    "\n",
    "If you do not use `.requires_grad(False)`, then:\n",
    "\n",
    "1. **Gradient Computation**: The model will unnecessarily compute gradients for the positional encodings during backpropagation, even though they should remain constant.\n",
    "\n",
    "2. **Memory Usage**: This will increase memory usage because the computational graph will keep track of operations performed on the positional encodings for gradient calculation.\n",
    "\n",
    "3. **Training Efficiency**: It can potentially slow down training because of the extra computation and memory used for something that doesn't contribute to model learning.\n",
    "\n",
    "So, it's a good practice to set `.requires_grad` to `False` for tensors that are not supposed to change during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .blue {\n",
    "        background-color: #0074D9;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"blue\">\n",
    "    Question: which num should take dropout in the positonal encoding ?\n",
    "</div>\n",
    "\n",
    "\n",
    "The dropout rate in the positional encoding module is a hyperparameter that you can tune based on your specific use case and the behavior of your model during training. Commonly used dropout rates range from **0.1 to 0.5**. A rate of **0.1** means that each unit or connection has a 10% chance of being dropped, while a rate of **0.5** means a 50% chance.\n",
    "\n",
    "Choosing the right dropout rate often involves experimentation and validation. Start with a lower rate like **0.1** or **0.2**, and if you notice overfitting (where your model performs well on the training data but poorly on unseen data), you can try increasing the dropout rate to see if it improves generalization.\n",
    "\n",
    "Remember, too much dropout can also lead to underfitting, where the model fails to learn the underlying patterns in the data. It's all about finding the right balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 features:int,\n",
    "                 eps:float=10**-6\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps \n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # calculate the mean \n",
    "        mean = x.mean(dim=-1,\n",
    "                      keepdim=True)\n",
    "        \n",
    "\n",
    "        std = x.std(dim = -1, deepdim=True)\n",
    "\n",
    "        return self.alpha * (x - mean) / (std + self.eps) + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm = LayerNormalization(\n",
    "    features=512\n",
    ")\n",
    "norm.alpha\n",
    "norm.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .blue {\n",
    "        background-color: #0074D9;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"blue\">\n",
    "    Question: what number should give feature in this layernormalization ?\n",
    "</div>\n",
    "\n",
    "\n",
    "The `features` parameter in the `LayerNormalization` class should match the number of features (or dimensions) in the last axis of the input tensor that you want to normalize. For example, if you're working with a transformer model and your input tensor has a shape of `[batch_size, seq_length, d_model]`, where `d_model` is the size of the model's embeddings or hidden states, then `features` should be set to `d_model`.\n",
    "\n",
    "In other words, if your input tensor's last dimension has 512 features (i.e., each token in your sequence is represented by a 512-dimensional vector), then you would initialize your `LayerNormalization` module with `features=512`. This ensures that layer normalization is applied correctly across each feature dimension for every token in your sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model:int,\n",
    "                 d_ff:int,\n",
    "                 dropout:float\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "        return x \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeedForwardBlock(\n",
       "  (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedforward = FeedForwardBlock(d_model=512,\n",
    "                               d_ff=2048,\n",
    "                               dropout=0.2\n",
    "                               )\n",
    "feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model:int,  # 512\n",
    "                 h:int,        # 8\n",
    "                 dropout:float  # 0.2\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h \n",
    "        assert d_model % h == 0\n",
    "\n",
    "        self.d_k = d_model // h \n",
    "       \n",
    "        self.w_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.w_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout:nn.Dropout):\n",
    "\n",
    "        d_k = query.shape[-1]\n",
    "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask == 0,\n",
    "                                          -1e9)\n",
    "            \n",
    "        attention_scores = attention_scores.softmax(dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "            return (attention_scores @ value), attention_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "\n",
    "        query = query.view(\n",
    "            query.shape[0], query.shape[1], self.h, self.d_k\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        key = key.view(\n",
    "            key.shape[0], key.shape[1], self.h, self.d_k\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        value = value.view(\n",
    "            value.shape[0], value.shape[1], self.h, self.d_k\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        # calculation of attention \n",
    "        x , self.attention_score = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        return self.w_o(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttentionBlock(\n",
       "  (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiHeadAttention = MultiHeadAttentionBlock(\n",
    "    d_model=512,\n",
    "    h=8,\n",
    "    dropout=0.2\n",
    ")\n",
    "# multiHeadAttention.attention\n",
    "# multiHeadAttention.w_q\n",
    "# multiHeadAttention.w_k\n",
    "# multiHeadAttention.w_v\n",
    "# multiHeadAttention.w_o\n",
    "# multiHeadAttention.d_model\n",
    "# multiHeadAttention.h\n",
    "# multiHeadAttention.d_k\n",
    "multiHeadAttention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .blue {\n",
    "        background-color: #0074D9;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"blue\">\n",
    "    Question: what is the heads of multhhead attention ?\n",
    "</div>\n",
    "\n",
    "\n",
    "In multi-head attention, \"heads\" refer to the parallel attention mechanisms that run at the same time. Each head computes its own set of attention scores independently, allowing the model to capture different types of information from different parts of the input sequence. For example, one head might focus on the syntactic aspects while another might capture semantic information.\n",
    "\n",
    "The number of heads is a hyperparameter that you can tune. More heads allow the model to capture a wider variety of information but also increase computational complexity. Common values for the number of heads are 8 or 16, but this can vary based on your specific task and model size.\n",
    "\n",
    "Here's a simplified explanation:\n",
    "- **Single Attention**: One set of attention scores, focusing on one aspect of the input.\n",
    "- **Multi-Head Attention**: Multiple sets of attention scores (heads), each focusing on different aspects of the input simultaneously.\n",
    "\n",
    "This design is part of what allows transformers to be so effective for tasks involving sequences, such as language modeling and machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResidualConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 features:int,\n",
    "                 dropout:float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dropout(p=0.2, inplace=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residualconnection = ResidualConnection(\n",
    "    features=512,\n",
    "    dropout=0.2\n",
    ")\n",
    "residualconnection.dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EncoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 features:int,   # 512\n",
    "                 self_attention_block:MultiHeadAttentionBlock,\n",
    "                 feed_forward_block:FeedForwardBlock,\n",
    "                 dropout:float  # 0.2\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n",
    "\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))    \n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (residual_connections): ModuleList(\n",
       "    (0-1): 2 x ResidualConnection(\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (norm): LayerNormalization()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoderblock = EncoderBlock(\n",
    "    features=512,\n",
    "    dropout=0.2,\n",
    "    self_attention_block=MultiHeadAttentionBlock,\n",
    "    feed_forward_block=FeedForwardBlock\n",
    ")\n",
    "\n",
    "encoderblock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .blue {\n",
    "        background-color: #0074D9;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"blue\">\n",
    "    Question: (0-1): 2 x ResidualConnection   what is this ?\n",
    "</div>\n",
    "\n",
    "\n",
    "The notation \"(0-1): 2 x ResidualConnection\" indicates that there are two instances of the `ResidualConnection` module within the `ModuleList`, and they are indexed as 0 and 1. The \"2 x\" signifies that there are two copies or instances of the `ResidualConnection` module being used in sequence.\n",
    "\n",
    "In PyTorch's `ModuleList`, each module is indexed starting from 0, similar to elements in a Python list. So, this means you have two `ResidualConnection` modules, one after the other, which can be accessed using their respective indices (0 and 1) in the `ModuleList`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 features:int,  #512\n",
    "                 layers:nn.ModuleList\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (norm): LayerNormalization()\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(\n",
    "    features=512,\n",
    "    layers=12\n",
    ")\n",
    "encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the `Encoder` class you've provided, the `layers` parameter in the constructor should take a `nn.ModuleList` containing the layers or blocks that make up the encoder. Each element in this `ModuleList` should be a module that has a forward method accepting two arguments: `x` (the input data) and `mask`.\n",
    "\n",
    "For a transformer encoder, these layers are typically instances of an encoder block that might include:\n",
    "\n",
    "- Multi-head self-attention mechanism\n",
    "- Feed-forward neural network\n",
    "- Residual connections\n",
    "- Layer normalization\n",
    "\n",
    "Here's an example of how you might initialize the `layers` parameter with encoder blocks:\n",
    "\n",
    "\n",
    "\n",
    "In this example:\n",
    "- `d_model` is the number of features (e.g., 512).\n",
    "- `n_heads` is the number of attention heads.\n",
    "- `d_ff` is the dimensionality of the feed-forward network.\n",
    "- `dropout` is the dropout rate.\n",
    "- `n_layers` is the number of encoder blocks you want to stack.\n",
    "\n",
    "Each `EncoderBlock` would be defined to include multi-head attention, feed-forward network, and any necessary residual connections and layer normalizations. The exact structure would depend on your implementation of `EncoderBlock`.\n",
    "\n",
    "\n",
    "<style>\n",
    "    .blue {\n",
    "        background-color: #0074D9;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"blue\">\n",
    "    Question: how many value take n_layers ?\n",
    "</div>\n",
    "\n",
    "\n",
    "The value of `n_layers` depends on how deep you want your encoder to be. It represents the number of encoder blocks you will stack in your `ModuleList`. In transformer models, it's common to have anywhere from 2 to 12 layers, but this can vary depending on the complexity of the task and the amount of data you have.\n",
    "\n",
    "For example, if you're working with a smaller dataset or a less complex problem, you might choose a smaller number of layers, like 2 or 4. For larger datasets or more complex problems, you might use 6, 8, or even more layers.\n",
    "\n",
    "Ultimately, the choice of how many layers to use is a hyperparameter that you can tune during the training process. You might start with a certain number and then increase or decrease it based on the performance of your model on validation data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 features:int,\n",
    "                 self_attention_block:MultiHeadAttentionBlock,\n",
    "                 cross_attention_block:MultiHeadAttentionBlock,\n",
    "                 feed_forward_block:FeedForwardBlock,\n",
    "                 dropout\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block,\n",
    "        self.feed_feedward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n",
    "\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "\n",
    "        x = self.residual_connections[0](x, lambda x:self.self_attention_block(x, x, x, tgt_mask))\n",
    "        x = self.residual_connections[0](x, lambda x:self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "        x = self.residual_connections[2](x, self.feed_feedward_block)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderBlock(\n",
       "  (residual_connections): ModuleList(\n",
       "    (0-2): 3 x ResidualConnection(\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "      (norm): LayerNormalization()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = DecoderBlock(\n",
    "    features=512,\n",
    "    self_attention_block=MultiHeadAttentionBlock,\n",
    "    cross_attention_block=MultiHeadAttentionBlock,\n",
    "    feed_forward_block=FeedForwardBlock,\n",
    "    dropout=0.2\n",
    ")\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 features:int,\n",
    "                 layers:nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "\n",
    "    def forward(self, \n",
    "                x, \n",
    "                encoder_output,\n",
    "                src_mask, \n",
    "                tgt_mask):\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (norm): LayerNormalization()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(\n",
    "    features=512,\n",
    "    layers=12)\n",
    "\n",
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProjectionLayer(\n",
       "  (proj): Linear(in_features=512, out_features=200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearLayer = ProjectionLayer(\n",
    "    d_model=512,\n",
    "    vocab_size=200\n",
    ")\n",
    "linearLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder:Encoder,\n",
    "                 decoder:Decoder,\n",
    "                 src_embed = EnputEmbedding,\n",
    "                 tgt_embed = EnputEmbedding,\n",
    "                 src_pos = PositionalEncoding,\n",
    "                 tgt_pos = PositionalEncoding,\n",
    "                 projection_layer = ProjectionLayer\n",
    "                 ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "\n",
    "    def encode(self, \n",
    "               src,  # source language\n",
    "               src_mask\n",
    "               ):\n",
    "        \n",
    "        src = self.src_embed(src)\n",
    "        src = self.src_pos(src)\n",
    "        return self.encoder(src, src_mask)\n",
    "    \n",
    "\n",
    "    def decode(self, \n",
    "               encoder_output:torch.Tensor,\n",
    "               src_mask:torch.Tensor,\n",
    "               tgt:torch.Tensor,\n",
    "               tgt_mask:torch.Tensor\n",
    "               ):\n",
    "        \n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "\n",
    "        return self.decoder(tgt, # value\n",
    "                            encoder_output,  # query, key\n",
    "                            src_mask, tgt_mask)\n",
    "    \n",
    "\n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = Transformer(\n",
    "    encoder=Encoder,\n",
    "    decoder=Decoder,\n",
    "    src_embed=EnputEmbedding,\n",
    "    tgt_embed = EnputEmbedding,\n",
    "    src_pos = PositionalEncoding,\n",
    "    tgt_pos = PositionalEncoding,\n",
    "    projection_layer=ProjectionLayer\n",
    ")\n",
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(\n",
    "        src_vocab_size:int,  # source language\n",
    "        tgt_vocab_size:int,  # target language\n",
    "        src_seq_len:int,\n",
    "        tgt_seq_len:int,\n",
    "        d_model:int=512,\n",
    "        N:int=6,             # Number of layers\n",
    "        h:int=8,              # number of heads\n",
    "        dropout:float=0.1,\n",
    "        d_ff:int=2048) -> Transformer:\n",
    "    \n",
    "\n",
    "    src_embed = EnputEmbedding(d_model, src_vocab_size)\n",
    "    tgt_embed = EnputEmbedding(d_model, tgt_vocab_size)\n",
    "\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
    "    tgt_pos= PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
    "\n",
    "\n",
    "    encoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        encoder_block = EncoderBlock(d_model, encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block)\n",
    "\n",
    "    \n",
    "    decoder_blocks = []\n",
    "    for _ in range(N):\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, \n",
    "                                     decoder_self_attention_block,\n",
    "                                     decoder_cross_attention_block,\n",
    "                                     feed_forward_block,\n",
    "                                     dropout)\n",
    "        \n",
    "        decoder_blocks.append(decoder_block)\n",
    "\n",
    "\n",
    "\n",
    "    # create the encoder the decoder \n",
    "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
    "\n",
    "\n",
    "    # create the projection layer \n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "\n",
    "    # create transformer \n",
    "    transformer = Transformer(encoder,\n",
    "                              decoder,\n",
    "                              src_embed,\n",
    "                              tgt_embed,\n",
    "                              src_pos,\n",
    "                              tgt_pos,\n",
    "                              projection_layer)\n",
    "    \n",
    "\n",
    "\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "\n",
    "    return transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder): Encoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x EncoderBlock(\n",
       "        (self_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (feed_forward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-1): 2 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x DecoderBlock(\n",
       "        (self_attention_block): MultiHeadAttentionBlock(\n",
       "          (w_q): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_k): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_v): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (w_o): Linear(in_features=512, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (feed_feedward_block): FeedForwardBlock(\n",
       "          (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (residual_connections): ModuleList(\n",
       "          (0-2): 3 x ResidualConnection(\n",
       "            (dropout): Dropout(p=0.2, inplace=False)\n",
       "            (norm): LayerNormalization()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNormalization()\n",
       "  )\n",
       "  (src_embed): EnputEmbedding(\n",
       "    (embeddings): Embedding(22, 512)\n",
       "  )\n",
       "  (tgt_embed): EnputEmbedding(\n",
       "    (embeddings): Embedding(22, 512)\n",
       "  )\n",
       "  (src_pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (tgt_pos): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (projection_layer): ProjectionLayer(\n",
       "    (proj): Linear(in_features=512, out_features=22, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = build_transformer(\n",
    "    src_vocab_size=22,\n",
    "    tgt_vocab_size=22,\n",
    "    tgt_seq_len=40,\n",
    "    src_seq_len=40,\n",
    "    d_model=512,\n",
    "    N=6,\n",
    "    h=8,\n",
    "    dropout=0.2,\n",
    "    d_ff=2048\n",
    ")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
