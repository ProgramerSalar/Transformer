{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config, get_weights_file_path, latest_weights_file_path\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "from config import get_config, get_weights_file_path, latest_weights_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset                    # we will use the dataset \n",
    "from tokenizers import Tokenizer                      # tokenizer library in huggingface\n",
    "from tokenizers.models import WordLevel                # we will use the word lavel tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer        # that class will train the tokenizer \n",
    "from tokenizers.pre_tokenizers import Whitespace         # we will split the word according to the white space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tokenizers\n",
    "- Most of the code taken from https://huggingface.co/docs/tokenizers/quicktour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will get all senteces so that we can iterate through the dataset to get all the sentences corresponding to the part the particular language \n",
    "# for which we are creating the tokenizer \n",
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:    # each item of dataset is pair of sentences extract the one particular language \n",
    "        yield item['translation'][lang]   # this is the item representing the pair and from this pair we extract only the one language thats we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(config,     # configuration of our model \n",
    "                           ds,          # dataset \n",
    "                           lang):        # language which language convert the token\n",
    "    \n",
    "    # that file where we will save this tokenizer \n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))   # path of configuration file name is tokenizer_file and which format of language\n",
    "    if not Path.exists(tokenizer_path):             # if path is exists \n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))    # unk means unknown, if our tokenizer sees a word that it does not recogized in its vocabulary it will replace it with this word unknown it will map it to the number corresponding to this word unknown \n",
    "        tokenizer.pre_tokenizer = Whitespace           # that we wplit by white space \n",
    "        \n",
    "        # then we train we build the trainer to train our tokenizer \n",
    "        trainer = WordLevelTrainer(   # this is word lavel trainer it will split the word using the white space and also single word\n",
    "            special_tokens = [\"[UNK]\",     # unknown word \n",
    "                              \"[PAD]\",     # padding of word \n",
    "                              \"[SOS]\",     # start of the sentence \n",
    "                              \"[EOS]\"],    # end of the sentence it has to have a frequency of at least two \n",
    "            min_frequency=2)   # that word for a word to appear in our vocabulary \n",
    "        \n",
    "        tokenizer.train_from_iterator(get_all_sentences(  #  give all the sentences from dataset \n",
    "            ds,         # dataset \n",
    "            lang),      # language \n",
    "            trainer=trainer)   # word lavel trainer \n",
    "        tokenizer.save(str(tokenizer_path))    # save the tokenizer_path\n",
    "        \n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Get Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from dataset import BilingualDataset, causal_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note- in this line \n",
    "    ```train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])```\n",
    "\n",
    "    we will create the dataset that is BilingualDataset dataset, so you go to dataset.py file see BilingualDataset dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(config):  # get dataset which also data takes the configuation of the model\n",
    "    \n",
    "    # it only has the train split, so we divide it overselves \n",
    "    # huggingface allow us to download its very easily we just need to tell him what is the name of the dataset and tell him what is the subset we want \n",
    "    # the subset that is English to italian but we want to also make it configurable for you guys to change the language very fast so \n",
    "    ds_raw = load_dataset(  \n",
    "        'opus_books',   # what is the name of the dataset \n",
    "        f\"{config['lang_src']}-{config['lang_tgt']}\",      # subset of the dataset, we will have two parameters in the configuration one is called language source and one is called language target\n",
    "        split='train')   # leter defined what split we want in dataset in our case only training split we want \n",
    "    \n",
    "    \n",
    "    # Build tokenizers \n",
    "    tokenizer_src = get_or_build_tokenizer(config,    # configuration \n",
    "                                           ds_raw,    # Raw dataset\n",
    "                                           config['lang_src'])  # source language of tokenizer \n",
    "    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "    \n",
    "    \n",
    "    # keep 90% for training 10% for validation \n",
    "    train_ds_size = int(0.9 * len(ds_raw))          # this is train data \n",
    "    val_ds_size = len(ds_raw) - train_ds_size       # this is validatio data \n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_raw, val_ds_raw])   # the method random split allow it's a method from pytorch that allow to split a dataset using the size that we give as input\n",
    "    \n",
    "    \n",
    "    # we need to create the dataset the dataset theat our model will use to access the tensor directory because before we just created the tokenizer and we just loaded the data but \n",
    "    # we need to create the tensors that our model will use \n",
    "    # create the BilingualDataset dataset \n",
    "    \n",
    "    \n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    \n",
    "    \n",
    "    # find the maximum length of each sentence in the source and target sentence \n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    # basically we do each sentence from each language from the source and the target language i convert into IDs using the tokenizer and \n",
    "    # i check the length if the length is let's say 190 we can choose 200 as sequence length because it will cover all the possible sentences\n",
    "    for item in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids \n",
    "        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids \n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "        \n",
    "        \n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "    \n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,   # train data \n",
    "        batch_size=config['batch_size'],  # batch size according to configuration\n",
    "        shuffle=True)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import build_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(config, \n",
    "              vocab_src_len, \n",
    "              vocab_tgt_len):\n",
    "    \n",
    "    model = build_transformer(\n",
    "        vocab_src_len,             # vocabulary source length\n",
    "        vocab_tgt_len,             # vocabulary target length\n",
    "        config[\"seq_len\"],        # sequence length of source language\n",
    "        config[\"seq_len\"],         # sequence length of target language \n",
    "        d_model=config[\"d_model\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \n",
    "    # Define the device \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    print(\"Using device:\", device)\n",
    "    if (device == 'cuda'):\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n",
    "    elif (device == 'mps'):\n",
    "        print(f\"Device name: <mps>\")\n",
    "    else:\n",
    "        print(\"NOTE: If you have a GPU, consider using it for training.\")\n",
    "        print(\"      On a Windows machine with NVidia GPU, check this video: https://www.youtube.com/watch?v=GMSjDTU8Zlc\")\n",
    "        print(\"      On a Mac machine, run: pip3 install --pre torch torchvision torchaudio torchtext --index-url https://download.pytorch.org/whl/nightly/cpu\")\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    \n",
    "     # Make sure the weights folder exists\n",
    "    Path(f\"{config['datasource']}_{config['model_folder']}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "    model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
    "    \n",
    "    \n",
    "# since we also have the configuration that allow us to resume the trining in case the model crashes  or something crashes \n",
    "# and that allow us to restore the state of the model and the state of the optimizer \n",
    "    # If the user specified a model to preload before training, load it\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "    if model_filename:\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(\n",
    "        ignore_index=tokenizer_src.token_to_id('[PAD]'),   # what is the ignore index  its padding token we don't want the loss to the padding token to contribute to the loss \n",
    "        # label smothing basically allows us our model to be less confident about its decision so imagine our model is telling us to choose the word number three and with very high probability so what we will do with label booting is take a little percentage of that probabilty and distribute to the other tokens so our model became less sure of its choice so kind of less overfit and this actually improves the accuracy of the model \n",
    "        label_smoothing=0.1   # so we will use the level putting of 0.1 which means from every highest probability token take 0.1 percent of score and give it the others \n",
    "        ).to(device)\n",
    "    \n",
    "    \n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "            # finally we get the tensor \n",
    "            encoder_input = batch['encoder_input'].to(device) # (b, seq_len) b -> batch size \n",
    "            decoder_input = batch['decoder_input'].to(device) # (B, seq_len)\n",
    "            encoder_mask = batch['encoder_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device) # (B, 1, seq_len, seq_len)   # why these two mask are diffrent because in the one case we are only telling him to hide only the padding tokens in the other case we are also telling him to hide all this subsequent words for each word to hide all the subsequent words to mask them out \n",
    "\n",
    "            # Run the tensors through the encoder, decoder and the projection layer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask) # (B, seq_len, d_model)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask) # (B, seq_len, d_model)\n",
    "            proj_output = model.project(decoder_output) # (B, seq_len, vocab_size)   # Linear layer \n",
    "\n",
    "            # Compare the output with the label\n",
    "            label = batch['label'].to(device) # (B, seq_len)  # what is label -> label is each position tell each B and sequence length so for each dimension tells us what is the position in the vocabulary of that particular word  \n",
    "\n",
    "            # Compute the loss using a simple cross entropy\n",
    "            # (B, seq_len, tgt_vocab_size)  --> (B * seq_len, tgt_vocab_size)\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "            \n",
    "            \n",
    "            # Log the loss\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backpropagate the loss\n",
    "            loss.backward()     # loss of the backward if loss is more the start for training for bigning \n",
    "\n",
    "            # Update the weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)    # zero grad \n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Run validation at the end of every epoch\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        # Save the model at the end of every epoch\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        \n",
    "        \n",
    "        # when we want to be able to resume the training to also save not only the state of the model but also the state of the optimizer because optimizer also keep tracks of some statistics one for each weight to understand how to move each weight so independently so i saw the optimizer dictionary is very big if you want your training to be resumable you need to save it otherwise the optimizer will always start from zero and will have to figure out from zero , so every time you save some snapshot \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step\n",
    "        }, model_filename)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")   # warning is frustating wo i want to filter them out \n",
    "    config = get_config()\n",
    "    train_model(config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
